{"content": "We are excited to announce our newest data extraction API. The  is now publicly available as a BETA release.If you want to skip the introductions and just get stuck in, here are the links you need:AutoExtract Comments API sets out to bring the power of our automatic data extraction capabilities currently used for applications such as  and more into the arena of blog comment analysis.\u00a0The underlying data model for the API was released to production as part of 20.6.0 release of AutoExtract.Customer support management presents many challenges due to the sheer number of requests, varied topics, and diverse departments within a company that might have a say in resolving the matter.Sourcing structured data from blog comments as provided by our API can be used in tandem with  solutions to quickly and effectively identify, track and act upon particular conversation strings \u2018hidden\u2019 amongst the noise of thousands of comments. You are effectively highlighting warning signs that your CX team should become involved before an incident takes place.Another particular powerful insight that can be derived from comments revolving around the sphere of Voice of Customer (VoC) and product analysis. By tapping into blog comments, you can search keywords for a particular product or feature or use the parsed data to train sentiment analysis model to find only the information you need."}
{"content": "Imagine a long crawling process, like extracting data from a website for a whole month. We can start it and leave it running until we get the results. Though, we can agree that a whole month is plenty of time for something to go wrong. The target website can go down for a few minutes/hours, there can be some sort of power outage in your crawling server or even some other internet connection issues.Any of those are real case scenarios and can happen at any given moment, bringing risk to your data extraction pipeline. In this case, if something like that happens, you may need to restart your crawling process and wait even longer to get access to that precious data. But, you don\u2019t need to panic, this is where (HCF) comes to our rescue.HCF is an API to store request data and is available through Scrapy Cloud projects. It is a bit similar to , but its intended use is to store request data, not a generic key value storage like Collections. At this moment, if you are familiar with , you may be wondering why one would use HCF, when Scrapy can store and recover the crawling state by itself.\u00a0The advantage is that Scrapy requires you to manage this state, by saving the content to disk (so needs disk quota) and if you are running inside a container, like in Scrapy Cloud, local files are lost once the process is finished. So, having some kind of external storage for requests is an alternative that takes this burden from your shoulders, leaving you to think about the extraction logic and not about the details on how to proceed in case it crashes and you need to restart.Before digging into an example of how to use HCF, I\u2019ll go over a bit on how it is structured. We can create many Frontiers per project, for each one we need a name. These Frontiers are then broken into slots, something similar to sharding, that can be useful in a producer-consumer scenario (topic of one of our upcoming blog posts). Usually, the name will be the name of the spider, to avoid any confusion. The catchy part is that we shouldn't change the number of slots after it was created, so keep it in mind when creating it.Now that we know what HCF is and how we could make use of it, it is time to see it working. For this purpose, we\u2019ll build a simple Scrapy spider to extract book information from . To get started, we\u2019ll create a new scrapy project and install the proper dependencies as shown below (type them in your terminal)."}
{"content": "Price Intelligence is leveraging web data to make better pricing, marketing, and business decisions. Basically, it is all about making use of the available data to optimize your pricing strategy, making it more competitive, increasing profitability, and ultimately, improving your business performance.From competitor monitoring to dynamic pricing and MAP monitoring, web extracted pricing data has endless uses. Brands and e-commerce companies use pricing data to monitor an overall view of the market. Dynamic pricing can be used to make automatic pricing decisions based on competitor\u2019s data combined with internal data so that you always remain profitable. MAP or Minimum Advertising Price monitoring is a technique that uses web extracted data to ensure the resellers and partners are maintaining the pricing according to the company policies.During our webinar on \u201c\u201d in June 2020, we got a lot of questions related to the processes and challenges of pricing data extraction. We cover a few important questions here:A: It varies from website to website, but the general idea is to find the pages where such promotion codes are available and build the logic of looking up code and applying it (clicking a button or sending AJAX request) into your extraction code.A: Websites showcase erroneous pricing data when they detect you scraping regularly. This especially happens when you are trying to scale - i.e scrape a lot of products very frequently. Erroneous pricing is not easily recognizable, but comparing the prices or other data fields with previously extracted data and manually checking if there is a big difference in the extracted data can help.The long-term solution for this would be to be smarter about how you scale and be more thoughtful about the  you use.A: Scraping accurate data is all about having a reliable quality assurance process. The first step towards this process is to have a well-defined JSON schema. Your QA process needs to be a balanced combination of automated ways of testing the data as well as manual ways. This blog post gives a detailed description of ."}
{"content": "The manual way or the highway...In software testing and QA circles, the topic of whether automated or manual testing is superior remains a hotly debated one. For  QA and validation specifically, they are not mutually exclusive. Indeed, for data, manual QA can inform automated QA, and vice versa. In this post, we\u2019ll give some examples.It is rare that can be adequately validated with automated techniques alone; additional manual inspections are often needed. The optimal blend of manual and automated tests depends on factors including:When considered in isolation, each have their benefits and drawbacks:, when rules are clearly defined and relatively static, this includes things like:, on the other hand, are invaluable for a deeper understanding of suspected data quality problems, particularly for data extracted from dynamic e-commerce websites and marketplaces.\u00a0From a practical point of view, the validation process should start with an understanding of the data and its characteristics. Next, define what rules are needed to validate the data, and automate them. The results of the automation will be warnings and possible false alarms that need to be verified using manual inspection. After the improvement of the rules, the second iteration of automated checks can be executed.Let's suppose we have the task of verifying the extraction coverage and correctness for this website: "}
{"content": "As the COVID-19 pandemic took hold, we at Scrapinghub began to wonder how it would impact on the data we crawl, and whether that data could tell us something useful about the pandemic and its impact.Retail price intelligence is one of our key areas of interest. On a daily basis,  for price and stock level information. What insights might be hidden in that data?To explore this, we identified a basket of goods related to pandemic preparedness (eg Face Masks, medications, etc) and began to track the number of individual items available online in this set, and the average item price over time.In the first chart, you can see the overall price (in blue), and the number of individual items (SKU\u2019s, in retail parlance, in red) over time for a collection of US data sources. You can see there is a lot of variation in the number of items on supply as new vendors enter the market with basic items. As you might expect, the price often moves in the opposite direction, as in the dip around April 5th. Basic economics works, but the average price doesn\u2019t drop away as strongly as you might expect. Underlying demand is clearly strong. Note that we aren\u2019t looking at stock levels here, just the range of items in stock and available for purchase. Variation in the number of items available could be due to opportunistic vendors entering the market, or rebranding routine supplies with COVID-19 related keywords.The strength of the underlying demand is such that the overall average price on the basket increased over 40% from March 29th (when we began to track the series) onwards over two months to May 28th. COVID-19 case counts in the US began to lift off in Mid-March, and with  leading that we would expect the \u2018normal\u2019 basket price had already risen considerably from its pre-COVID-19 base when we began to track the data.\u00a0The interactive map shows the % increase is the average item cost between 1st April and 1st June 2020. You can see that even in a homogeneous market like the US, there is considerable state by state variation in the amount of price increases, with prices more than doubling in some states (Alabama, Wyoming) but showing very modest increases in others (Minnesota). The increases do not seem to correlate with actual observed COVID-19 case levels.The second chart shows the price, overall, and 3 US states, for comparison. You can see the price \u2018wobbles\u2019 quite a lot, often with different states not quite in sync. We expected to see lower prices in states with less active cases, but that doesn\u2019t seem to be the case at all, as the fluid US internal market smooths out price variation within a few days, and thus no systematic variation in price."}
{"content": "Article and news data extraction is becoming increasingly popular and widely used by companies. Data quality plays a vital role in making sure these projects succeed. If the quality of the extracted articles is not good enough, your whole business could be at risk, especially if it depends on the constant flow of high quality article data.When it comes to web data extraction, data quality is always a key factor. Without high data quality, organizations face increased costs () let alone having their competitive standing undermined. If you\u2019re looking for an article extraction solution, your top priority should be data quality. You need to know which service or library provides the best article data quality. You need to learn what metrics are important when you . But also - moving further from general data quality - what measures are important in article extraction and article body extraction quality.Article body extraction quality is crucial if your business depends on this kind of data. If you\u2019re developing a product or software that needs structured article/news data constantly, you need to make sure you choose a solution which can prove they provide the best quality on the market.There are many use cases for article extraction. But one thing is common in each of them: extracting articles from the web gives you a competitive advantage that many companies fail to recognize yet. Web extracted articles and news can make youIf you want any of these skills in your arsenal, your top priority should be to choose a solution that has the best article extraction quality on the market.If you have products sold online, there\u2019s probably a lot of discussion around them as well.  their good or bad experiences of a product they bought. These mentions can decide whether future customers buy from you or they choose another brand\u2019s product. Monitoring your brand online and fueling mentions into your business intelligence can improve the way you market, promote and present your products online. It can also show you why people are buying (or not buying) your products."}
{"content": "The web is complex and constantly changing. It is one of the reasons why web data extraction can be difficult, especially in the long term. It\u2019s necessary to understand how a website works really well, before you try to extract data. Luckily, there are lots of inspection and code tools available for this and in this article we will show you some of our favorites.All major browsers come packed with a set of development tools. Although these have been built with the goal of building websites in mind, they can also be used to analyze web pages and traffic. These are some pretty powerful tools for working with websites.For Google Chrome, these developer tools can be accessed from any page by right-clicking then choosing 'Inspect' or using the shortcut 'CTRL + shift + I' (or '\u2318 + Option + I' on macs).You can use these tools to perform some basic operations:Most web pages contain a lot of javascript that needs to be executed before you can see the final output. But you can see how the initial request looks before all of this by checking the page source. To do that you can right-click and then click on 'View page source' or use the shortcuts:Press CTRL + U (\u2318 + Option + U) to view sourcePress CTRL + F (\u2318 + Option + F) to searchSometimes the information you're looking for is not loaded with the first request. Or perhaps there is an API call that loads the data you need. In order to \u201ccatch\u201d these calls, you'll want to see what requests are needed to load a certain page. You can find out this and more in the 'Network' tab. This is what it looks like:Of course, you'll also need to know the details on individual requests. If you click on any of these you'll be able to see a lot more information such as request and response headers, cookies and the payload used when sending POST requests for example.Let's take a look at one of the requests needed to load the main page of google."}
{"content": "We are excited to announce our newest data extraction API. The  is now out of BETA and publicly available as a stable release.\u00a0If you are ready to roll up your sleeves and get started, here are the links you need:While this blog covers most of the notable improvements & extensive testing that the API has undergone, that warrants an exit from Beta, together with some high-level uses; it\u2019s important to remember that we have already covered it .We are moving AutoExtract Job Postings out of beta after making substantial , completely eliminating several classes of errors, and making . Aggregator websites where the API had a tendency to return failed requests on the BETA release have now been addressed, paving the way for widespread use.These changes were released to production as part of 20.5.0If you are looking to discern insights on the activities of organisations of all sizes, from start-ups to Fortune 100 companies, job postings can provide context for analysts to understand the market landscape. Where and how are competitors, suppliers and customers or even the industry, in general, structuring their business. Which technologies they are investing in, which ones they are no longer actively pursuing, what key markets are they pushing into, amongst other things."}
{"content": "Generally, there are 3 steps needed to find the best proxy management method for your web scraping project and to make sure you can get data not just today but also in the future, long-term.You need to define the traffic profile first to determine the concrete needs of your project. What is a traffic profile?It includes, first of all, the  that you're trying to get data from. And also if there's any technical challenges needed to be solved, like JS rendering.The traffic profile also includes the , meaning how many requests do you want to or need to make per hour or per day. Also do you have any specific time window for the requests, like, for example you want to make all your requests only during work hours, for some reason. Or is it okay to get the data at night, when there's significantly less traffic hitting the site.Then the last thing in the traffic profile is, . Because sometimes the website displays different content depending on where you are. So you need to use proxies that are in that specific region you need.So these three elements together make the traffic profile: websites, volume and geo locations. Now, with these, you can determine the exact proxy situation that you need a solution for.The next step to scale up is to get a proxy pool. Based on the traffic profile, now you can estimate"}
{"content": "But, first let's see why would you even need proxies. When you start extracting data from the web on a small scale you might not need proxies to make successful requests and get the data. But, as you scale your project because you need to extract more records or more frequently, you will experience issues. Or the site you're trying to reach might display different content depending on the region. So these are the two cases when you need to start using a proxy solution. proxies are much easier to get access to and they are much cheaper. In many use cases, where you cannot extract data without any proxy, you can just start using data center proxies and be able to extract data.Residential proxies are harder to get access to and they are more expensive, because they are provided by actual Internet Service Providers and not data centers. Residential proxies are also higher quality and can work even when data center proxies fail.Whether you should use data center or residential proxies in your web data extraction project, it comes down to your situation\u2019s details. There\u2019s no general rule of thumb to decide which type of proxy will work for you. But one thing is for sure: unless you have some special requirements you should start off with data center proxies. Then, based on how it works for you, you can switch to residential proxies if you really need to.Residential proxies are more expensive, thus you will probably be better off using data center proxies, if you can, and applying some techniques to keep your proxy pool clean.The biggest issue with residential proxies is, as it was mentioned, they are expensive. So usually the most effective way to scale your web data extraction project, is to try to maximize the value of data center proxies, by being smart about how you actually scrape the web and how you use proxies.Two things, that you can do to achieve this:If you want to learn more about these tactics, I recommend watching our FREE webinar on this: If you missed our webinar on the topic of data center proxies and residential proxies don\u2019t worry you will be able to watch it here:If you feel like you know enough already and you don't want to spend way too much time on managing proxies, you can just use an already existing solution for "}
